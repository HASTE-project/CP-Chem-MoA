{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOCTxJS5_fM9"
      },
      "outputs": [],
      "source": [
        "# 10 selected MoAs \n",
        "moas_to_use = ['Aurora kinase inhibitor', 'tubulin polymerization inhibitor', 'JAK inhibitor', 'protein synthesis inhibitor', 'HDAC inhibitor', \n",
        "        'topoisomerase inhibitor', 'PARP inhibitor', 'ATPase inhibitor', 'retinoid receptor agonist', 'HSP inhibitor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lon5FqMB3JXq"
      },
      "outputs": [],
      "source": [
        "# read the data \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "all_data = pd.read_csv('../data_for_image_based_model/fl_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxOMrC-QANFg"
      },
      "outputs": [],
      "source": [
        "# Drop the lines with DMSO \n",
        "all_data = all_data[all_data.moa != 'dmso']\n",
        "assert 'dmso' not in all_data.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGKvY4IcANFh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJf3WSROANFh"
      },
      "outputs": [],
      "source": [
        "dictionary = {'ATPase inhibitor': 7, 'Aurora kinase inhibitor': 0,\n",
        " 'HDAC inhibitor': 4, 'HSP inhibitor': 9, 'JAK inhibitor': 2, 'PARP inhibitor': 6,\n",
        " 'protein synthesis inhibitor': 3, 'retinoid receptor agonist': 8,\n",
        " 'topoisomerase inhibitor': 5, 'tubulin polymerization inhibitor': 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alyBr650ANFh"
      },
      "outputs": [],
      "source": [
        "# change moa to classes \n",
        "all_data['classes'] = None\n",
        "for i in range(all_data.shape[0]):\n",
        "    all_data.iloc[i, 11] = dictionary[all_data.iloc[i, 10]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cZUzIZoANFi"
      },
      "outputs": [],
      "source": [
        "# add 'digit' column as index \n",
        "all_data['digit'] = [i for i in range(0, all_data.shape[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltfxRBgeANFi"
      },
      "outputs": [],
      "source": [
        "# shuffle the data \n",
        "all_data = all_data.sample(frac = 1, random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eukFs9dMANFi"
      },
      "outputs": [],
      "source": [
        "# get the compounds in test data  \n",
        "compound_test =  ['CBK303888', 'CBK200949', 'CBK309016', 'CBK290799', 'CBK308876', 'CBK289882',\n",
        " 'CBK308108', 'CBK290852', 'CBK307956', 'CBK290529', 'CBK309488', 'CBK290717', 'CBK277957', 'CBK290547',\n",
        " 'CBK291084', 'CBK289740', 'CBK278064', 'CBK308819', 'CBK308608', 'CBK288281', 'CBK309437', 'CBK289987',\n",
        " 'CBK278556G', 'CBK277961']\n",
        "assert len(compound_test) == 24 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKn1RfsmANFi"
      },
      "outputs": [],
      "source": [
        "# get the compounds in validation data \n",
        "compound_valid = ['CBK309483', 'CBK200593', 'CBK278120', 'CBK309545', 'CBK277955', 'CBK278047', 'CBK278016', 'CBK290481', 'CBK290217',\n",
        " 'CBK041182', 'CBK271643', 'CBK041804', 'CBK042154', 'CBK308980', 'CBK290803', 'CBK309258', 'CBK290154',\n",
        " 'CBK308260', 'CBK017131', 'CBK290770', 'CBK291055', 'CBK307747', 'CBK228145']\n",
        "assert len(compound_valid) == 23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S_m6q-nANFj"
      },
      "outputs": [],
      "source": [
        "# get the compounds in training data \n",
        "compound_train = []\n",
        "for i in all_data.compound.tolist():\n",
        "    if i not in (compound_test + compound_valid + compound_train):\n",
        "        compound_train.append(i)\n",
        "\n",
        "assert len(compound_train) + len(compound_valid) + len(compound_test) == len(set(all_data.compound.to_list()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73_IwUtfO7sm"
      },
      "outputs": [],
      "source": [
        "# get the train, valid and test set \n",
        "train = all_data[all_data['compound'].isin(compound_train)].reset_index(drop = True)  \n",
        "valid = all_data[all_data['compound'].isin(compound_valid)].reset_index(drop = True)  \n",
        "test = all_data[all_data['compound'].isin(compound_test)].reset_index(drop = True)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpaHDV_NyUTz"
      },
      "outputs": [],
      "source": [
        "# get the dictionary for compound_id-SMILES pair \n",
        "import pickle\n",
        "compound_smiles_dictionary = pickle.load(open(\"../data_for_image_based_model/dictionary2.pickle\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDGCruBJA1J2"
      },
      "outputs": [],
      "source": [
        "# on the fly data augmentation \n",
        "import albumentations as A\n",
        "train_transforms = A.Compose([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),\n",
        "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.2),\n",
        "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.4),\n",
        "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.5),\n",
        "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.6),\n",
        "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.8),\n",
        "    A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),])\n",
        "valid_transforms = A.Compose([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDZ2gf4WCxcW"
      },
      "outputs": [],
      "source": [
        "# get all images  \n",
        "import numpy as np\n",
        "all_images = np.load(open(, \"rb\")) # The path has to be set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOHasY7qCrm4"
      },
      "outputs": [],
      "source": [
        "# data generator for training \n",
        "def get_train_image(end):\n",
        "  start = 0\n",
        "  while start < end:      \n",
        "    idx = start       \n",
        "    row = train.iloc[idx]\n",
        "\n",
        "    assert row['digit']     in train.digit.tolist()\n",
        "    assert row['compound']   in train.compound.tolist()\n",
        "\n",
        "    assert row['digit']    not in valid.digit.tolist()\n",
        "    assert row['compound']  not in valid.compound.tolist()\n",
        "\n",
        "    assert row['digit']    not in test.digit.tolist()\n",
        "    assert row['compound']  not in test.compound.tolist()\n",
        "\n",
        "    image = all_images[all_data.digit.tolist().index(row['digit'])]  \n",
        "    image = train_transforms(image = image)['image']               \n",
        "    target = int(row['classes'])   \n",
        "    \n",
        "    yield image, target\n",
        "    start += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbPwaU-dCro5"
      },
      "outputs": [],
      "source": [
        "# data generator for validation   \n",
        "def get_valid_image(end):\n",
        "  start = 0\n",
        "  while start<end:\n",
        "    idx = start       \n",
        "    row = valid.iloc[idx]\n",
        "\n",
        "    assert row['digit']   not in train.digit.tolist()\n",
        "    assert row['compound']  not in train.compound.tolist()\n",
        "\n",
        "    assert row['digit']     in valid.digit.tolist()\n",
        "    assert row['compound']   in valid.compound.tolist()\n",
        "\n",
        "    assert row['digit']    not in test.digit.tolist()\n",
        "    assert row['compound']  not in test.compound.tolist()\n",
        "       \n",
        "    image = all_images[all_data.digit.tolist().index(row['digit'])] \n",
        "    image = valid_transforms(image = image)['image']\n",
        "    target = int(row['classes'])   \n",
        "    \n",
        "    yield image, target\n",
        "    start += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mz8y7LJCrrG"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "channels = 5      \n",
        "image_size = 256 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6agIQbDGkw3"
      },
      "outputs": [],
      "source": [
        "# turn to tensorflow datasets \n",
        "import tensorflow as tf\n",
        "train_data = tf.data.Dataset.from_generator(get_train_image,\n",
        "            (tf.float32, tf.int32),\n",
        "            ((tf.TensorShape([image_size, image_size, channels])), tf.TensorShape([])),\n",
        "            args = [train.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)\n",
        "\n",
        "valid_data = tf.data.Dataset.from_generator(get_valid_image,\n",
        "            (tf.float32, tf.int32),\n",
        "            ((tf.TensorShape([image_size, image_size, channels])), tf.TensorShape([])),\n",
        "            args = [valid.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPiZ1G_oG2Rl"
      },
      "outputs": [],
      "source": [
        "# we choose efficientnet b1 as the base model    \n",
        "base_model = tf.keras.applications.EfficientNetB1(input_shape = (image_size, image_size, channels),\n",
        "                                  include_top = False, weights = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-FX8pid1h_U"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbRdHCZx1h_U"
      },
      "outputs": [],
      "source": [
        "# complete the architecture of efficientnet b1\n",
        "drop = 0.30\n",
        "num_classes = len(set(train['classes'].tolist())) \n",
        "x = base_model.output\n",
        "x = Dropout(drop)(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = Dropout(drop)(x)\n",
        "preds = layers.Dense(num_classes, activation = 'softmax',                     \n",
        "    kernel_regularizer = regularizers.L1L2(l1 = 1e-4, l2 = 1e-3),\n",
        "    bias_regularizer = regularizers.L2(1e-3),\n",
        "    activity_regularizer = regularizers.L2(1e-4))(x)\n",
        "cnn_model = models.Model(inputs = base_model.input, outputs = preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd4PkWWJ1h_V"
      },
      "outputs": [],
      "source": [
        "# set the optimizer of efficientnet b1    \n",
        "cnn_optimizer = tfa.optimizers.AdamW(weight_decay = 1e-6, learning_rate = 0.001, beta_1 = 0.9,\n",
        "    beta_2 = 0.999, epsilon = 1e-07,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLZSv4icG6Qv"
      },
      "outputs": [],
      "source": [
        "# compile the model   \n",
        "cnn_model.compile(optimizer = cnn_optimizer,\n",
        "         loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "         metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2s1BvcYG--W"
      },
      "outputs": [],
      "source": [
        "# set the class weights \n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight(class_weight = 'balanced', \n",
        "         classes = np.unique(train.classes), y = train.classes)   \n",
        "weight_dictionary = dict(zip(np.unique(train.classes), class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buV4eg24HERe"
      },
      "outputs": [],
      "source": [
        "# set the check point   \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath_cnn = './content/shuffle_5_' + str(base_model.name) + '_weights.hdf5'\n",
        "checkpoint_cnn = ModelCheckpoint(filepath_cnn, monitor = 'val_accuracy', verbose = 0, \n",
        "                  save_best_only = True, mode = 'max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecAtvKNaHVec"
      },
      "outputs": [],
      "source": [
        "# train the efficientnet b1\n",
        "from tensorflow.keras.callbacks import EarlyStopping  \n",
        "reduce_lr_loss_cnn = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n",
        "              factor = 0.5, patience = 9, verbose = 2, min_lr = 1e-7, mode = 'min')\n",
        "history_cnn = cnn_model.fit(train_data, validation_data = valid_data, class_weight = weight_dictionary,\n",
        "               verbose = 2, epochs = 100, callbacks=[reduce_lr_loss_cnn, checkpoint_cnn,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATgS1vCJp9ko"
      },
      "outputs": [],
      "source": [
        "# get the best model \n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "best_cnn_model = load_model() # Path has to be set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPopQovGLhYp"
      },
      "outputs": [],
      "source": [
        "# evaluate the model   \n",
        "predicted_test = []\n",
        "for i in range(test.shape[0]):    \n",
        "  row = test.iloc[i]\n",
        "\n",
        "  assert row['digit']  not  in train.digit.tolist()\n",
        "  assert row['compound'] not  in train.compound.tolist()\n",
        "  \n",
        "  assert row['digit']   not  in valid.digit.tolist()\n",
        "  assert row['compound']  not  in valid.compound.tolist()\n",
        "\n",
        "  assert row['digit']     in test.digit.tolist()\n",
        "  assert row['compound']    in test.compound.tolist()  \n",
        "\n",
        "  im = all_images[all_data.digit.tolist().index(row['digit'])] \n",
        "  im = valid_transforms(image = im)['image']\n",
        "  im = np.expand_dims(im, 0)\n",
        "  value = best_cnn_model.predict(im).argmax()\n",
        "  \n",
        "  predicted_test.append(value)   \n",
        "  \n",
        "print(classification_report(test.classes.tolist(), predicted_test))    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "RVt4mcoaAb-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function changing SMILES to Morgan fingerprints \n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import DataStructs, AllChem\n",
        "def smiles_to_array(smiles):\n",
        "  molecules = Chem.MolFromSmiles(smiles) \n",
        "  fingerprints = AllChem.GetMorganFingerprintAsBitVect(molecules, 2)\n",
        "  x_array = []\n",
        "  arrays = np.zeros(0,)\n",
        "  DataStructs.ConvertToNumpyArray(fingerprints, arrays)\n",
        "  x_array.append(arrays)\n",
        "  x_array = np.asarray(x_array)\n",
        "  x_array = ((np.squeeze(x_array)).astype(int)) \n",
        "  return x_array"
      ],
      "metadata": {
        "id": "3nK7b0olARpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generator for training data \n",
        "def get_train_smiles(end):\n",
        "  start = 0\n",
        "  while start < end:\n",
        "    idx = start       \n",
        "    row = train.iloc[idx]\n",
        "\n",
        "    assert row['digit']    in train.digit.tolist()\n",
        "    assert row['compound']   in train.compound.tolist()\n",
        "  \n",
        "    assert row['digit']    not in valid.digit.tolist()\n",
        "    assert row['compound']  not  in valid.compound.tolist()\n",
        "\n",
        "    assert row['digit']   not  in test.digit.tolist()\n",
        "    assert row['compound']  not  in test.compound.tolist()\n",
        "\n",
        "    smiles = compound_smiles_dictionary[row['compound']]     \n",
        "    smiles_array = smiles_to_array(smiles)                      \n",
        "    target_mlp = int(row['classes'])   \n",
        "    \n",
        "    yield smiles_array, target_mlp\n",
        "    start += 1"
      ],
      "metadata": {
        "id": "7JpIlc_eARsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generator for validation data \n",
        "def get_valid_smiles(end):\n",
        "  start = 0\n",
        "  while start<end:\n",
        "    idx = start       \n",
        "    row = valid.iloc[idx]\n",
        "\n",
        "    assert row['digit']  not  in train.digit.tolist()\n",
        "    assert row['compound'] not  in train.compound.tolist()\n",
        "  \n",
        "    assert row['digit']     in valid.digit.tolist()\n",
        "    assert row['compound']    in valid.compound.tolist()\n",
        "\n",
        "    assert row['digit']   not  in test.digit.tolist()\n",
        "    assert row['compound']  not  in test.compound.tolist()\n",
        "\n",
        "    smiles = compound_smiles_dictionary[row['compound']]     \n",
        "    smiles_array = smiles_to_array(smiles)                  \n",
        "    target_mlp = int(row['classes'])   \n",
        "    \n",
        "    yield smiles_array, target_mlp\n",
        "    start += 1"
      ],
      "metadata": {
        "id": "726fS9zvARu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# complete the architecture of MLP and compile MLP \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "units = 64  \n",
        "drop = 0.89  \n",
        "\n",
        "model_mlp = Sequential()\n",
        "model_mlp.add(Dense(units, input_dim = 2048, activation = 'relu'))\n",
        "model_mlp.add(Dropout(drop))\n",
        "model_mlp.add(Dense(10, activation = 'softmax'))\n",
        "model_mlp.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4),\n",
        "         loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "         metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "4zVDC4aSARxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn to TensorFlow dataset \n",
        "train_smiles_data = tf.data.Dataset.from_generator(get_train_smiles,\n",
        "            (tf.float32, tf.int32),\n",
        "            (tf.TensorShape(2048), tf.TensorShape([])),\n",
        "            args = [train.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
      ],
      "metadata": {
        "id": "Jx2DIkZeARzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn to TensorFlow dataset  \n",
        "valid_smiles_data = tf.data.Dataset.from_generator(get_valid_smiles,\n",
        "            (tf.float32, tf.int32),\n",
        "            (tf.TensorShape(2048), tf.TensorShape([])),\n",
        "            args = [valid.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
      ],
      "metadata": {
        "id": "xUFIjD1WAvbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the checkpoint\n",
        "filepath_mlp = './content/shuffle_5_mlp_weights.hdf5'\n",
        "checkpoint_mlp = ModelCheckpoint(filepath_mlp, monitor = 'val_accuracy', verbose = 0, \n",
        "                  save_best_only = True, mode = 'max')"
      ],
      "metadata": {
        "id": "rW7jfZP3Avdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model  \n",
        "from tensorflow.keras.callbacks import EarlyStopping  \n",
        "earlyStopping = EarlyStopping(monitor = 'val_loss', patience = 20, verbose = 2, mode = 'min')\n",
        "reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n",
        "                factor = 0.5, patience = 9, verbose = 2, min_lr = 1e-7, mode = 'min')\n",
        "history_mlp = model_mlp.fit(train_smiles_data, validation_data = valid_smiles_data,\n",
        "               class_weight = weight_dictionary, verbose = 2, epochs = 1800,      \n",
        "               callbacks = [earlyStopping, reduce_lr_loss, checkpoint_mlp,])"
      ],
      "metadata": {
        "id": "b12O81E8Avgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the best MLP and evaluate it  \n",
        "#best_model_mlp = load_model('/content/drive/MyDrive/github/shuffle_5_mlp_weights.hdf5') \n",
        "best_model_mlp = load_model('') # Path has to be set\n",
        "predicted_test = []\n",
        "for i in range(test.shape[0]): \n",
        "  row = test.iloc[i]\n",
        "\n",
        "  assert row['digit']  not  in train.digit.tolist()\n",
        "  assert row['compound'] not  in train.compound.tolist()\n",
        "  \n",
        "  assert row['digit']   not  in valid.digit.tolist()\n",
        "  assert row['compound']  not   in valid.compound.tolist()\n",
        "\n",
        "  assert row['digit']     in test.digit.tolist()\n",
        "  assert row['compound']    in test.compound.tolist()\n",
        "  \n",
        "  smiles = compound_smiles_dictionary[row['compound']]     \n",
        "  smiles_array = smiles_to_array(smiles)  \n",
        "  smiles_array = np.expand_dims(smiles_array, 0)\n",
        "  value = int(best_model_mlp.predict(smiles_array)[0].argmax())\n",
        "\n",
        "  predicted_test.append(value) \n",
        "\n",
        "print(classification_report(test.classes.tolist(), predicted_test))      "
      ],
      "metadata": {
        "id": "NXJVS2HdAviw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate MLP and CNN.\n",
        "\n"
      ],
      "metadata": {
        "id": "RIyXc9OXA3XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data generator for the global model \n",
        "def get_train_image_smiles(end):\n",
        "  start = 0\n",
        "  while start < end:\n",
        "    idx = start       \n",
        "    row = train.iloc[idx]\n",
        "\n",
        "    assert row['digit']    in train.digit.tolist()\n",
        "    assert row['compound']   in train.compound.tolist()\n",
        "  \n",
        "    assert row['digit']   not  in valid.digit.tolist()\n",
        "    assert row['compound']  not  in valid.compound.tolist()\n",
        "\n",
        "    assert row['digit']   not  in test.digit.tolist()\n",
        "    assert row['compound']  not  in test.compound.tolist()\n",
        "\n",
        "    image  = all_images[all_data.digit.tolist().index(row['digit'])]  \n",
        "    image = train_transforms(image = image)['image'] \n",
        "\n",
        "    smiles = compound_smiles_dictionary[row['compound']]     \n",
        "    smiles_array = smiles_to_array(smiles)                \n",
        "    target = int(row['classes'])   \n",
        "\n",
        "    yield (image, smiles_array), target\n",
        "    start += 1"
      ],
      "metadata": {
        "id": "gyLphZ2fAvlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validation data generator for the global model \n",
        "def get_valid_image_smiles(end):\n",
        "  start = 0\n",
        "  while start < end:\n",
        "    idx = start       \n",
        "    row = valid.iloc[idx]\n",
        "       \n",
        "    assert row['digit']  not  in train.digit.tolist()\n",
        "    assert row['compound'] not  in train.compound.tolist()\n",
        "  \n",
        "    assert row['digit']     in valid.digit.tolist()\n",
        "    assert row['compound']    in valid.compound.tolist()\n",
        "\n",
        "    assert row['digit']   not  in test.digit.tolist()\n",
        "    assert row['compound']  not  in test.compound.tolist()       \n",
        "    \n",
        "    image = all_images[all_data.digit.tolist().index(row['digit'])] \n",
        "    image = valid_transforms(image = image)['image']\n",
        "    smiles = compound_smiles_dictionary[row['compound']]     \n",
        "    smiles_array = smiles_to_array(smiles)      \n",
        "    target = int(row['classes'])   \n",
        "\n",
        "    yield (image, smiles_array), target\n",
        "    start += 1"
      ],
      "metadata": {
        "id": "vCI5vdcDAvnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn to TensorFlow dataset \n",
        "train_image_smiles = tf.data.Dataset.from_generator(get_train_image_smiles,\n",
        "           ((tf.float32, tf.float32), tf.int32),\n",
        "           ((tf.TensorShape([image_size, image_size, channels]), 2048), tf.TensorShape([])),\n",
        "           args = [train.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
      ],
      "metadata": {
        "id": "9RSRKnXlAR2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn to TensorFlow dataset \n",
        "valid_image_smiles = tf.data.Dataset.from_generator(get_valid_image_smiles,\n",
        "           ((tf.float32, tf.float32), tf.int32),\n",
        "           ((tf.TensorShape([image_size, image_size, channels]), 2048), tf.TensorShape([])),\n",
        "           args = [valid.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
      ],
      "metadata": {
        "id": "7lWzCDU2A9CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine MLP and efficientNet to get the global model    \n",
        "from keras import Model\n",
        "model1 = Model(inputs = best_cnn_model.input, \n",
        "        outputs = best_cnn_model.get_layer('global_average_pooling2d').output)\n",
        "\n",
        "model2 = Model(inputs = best_model_mlp.input, outputs = best_model_mlp.get_layer('dropout_2').output)"
      ],
      "metadata": {
        "id": "9T0CpX2dA9F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = model1.output    \n",
        "x1 = tf.keras.layers.Dense(64, activation = 'relu')(x1)    \n",
        "x2 = model2.output \n",
        "x3 = tf.concat([x1, x2], axis = -1)\n",
        "x3 = tf.keras.layers.Dense(128, activation = 'relu')(x3)\n",
        "output1 = tf.keras.layers.Dense(10, activation = 'softmax')(x3)\n",
        "global_model = tf.keras.models.Model(inputs = [model1.input, model2.input], outputs = output1)"
      ],
      "metadata": {
        "id": "J9bKQId7A9Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze MLP and efficientnet \n",
        "model1.trainable = False\n",
        "model2.trainable = False"
      ],
      "metadata": {
        "id": "j5XIHzMhA9LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the global model        \n",
        "global_model.compile(optimizer = cnn_optimizer, \n",
        "           loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "           metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "GQvhtBPXBEBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the global model with MLP and efficientnet freezed     \n",
        "history_global = global_model.fit(train_image_smiles, validation_data = valid_image_smiles,     \n",
        "                  class_weight = weight_dictionary, verbose = 2, epochs = 10)"
      ],
      "metadata": {
        "id": "47lCpnv5BEEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze MLP and efficientNet   \n",
        "model1.trainable = True\n",
        "model2.trainable = True"
      ],
      "metadata": {
        "id": "iV27xVymBEGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the optimizer\n",
        "global_model_optimizer = tfa.optimizers.AdamW(weight_decay = 1e-6, learning_rate = 1e-3/2,\n",
        "                         beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07,)"
      ],
      "metadata": {
        "id": "4xrM6w6VBEIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the global model            \n",
        "global_model.compile(optimizer = global_model_optimizer, \n",
        "           loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "           metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "ujoSAxZfBELC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the checkpoint   \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = './content/shuffle_5_global_weights.hdf5'\n",
        "global_model_checkpoint = ModelCheckpoint(filepath, monitor = 'val_accuracy', verbose = 0, \n",
        "                      save_best_only = True, mode = 'max')"
      ],
      "metadata": {
        "id": "35YwZdLcAR4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the global model \n",
        "reduce_lr_loss_global = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n",
        "             factor = 0.5, patience = 9, verbose = 2, min_lr = 1e-7, mode = 'min')\n",
        "history_global = global_model.fit(train_image_smiles, validation_data = valid_image_smiles,     \n",
        "             class_weight = weight_dictionary, verbose = 2, epochs = 100-10,\n",
        "             callbacks = [reduce_lr_loss_global, global_model_checkpoint,])"
      ],
      "metadata": {
        "id": "vnBWLxS3BL2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the best global model and evaluate it \n",
        "#best_global_model = load_model('/content/drive/MyDrive/github/shuffle_5_global_weights.hdf5')\n",
        "best_global_model = load_model('') # Path has to be set\n",
        "predicted_test = []\n",
        "for i in range(test.shape[0]):\n",
        "  row = test.iloc[i]\n",
        "\n",
        "  assert row['digit']  not  in train.digit.tolist()\n",
        "  assert row['compound'] not  in train.compound.tolist()\n",
        "  \n",
        "  assert row['digit']    not in valid.digit.tolist()\n",
        "  assert row['compound']  not  in valid.compound.tolist()\n",
        "\n",
        "  assert row['digit']     in test.digit.tolist()\n",
        "  assert row['compound']    in test.compound.tolist()\n",
        "\n",
        "  image = all_images[all_data.digit.tolist().index(row['digit'])] \n",
        "  image = valid_transforms(image = image)['image']\n",
        "  smiles = compound_smiles_dictionary[row['compound']]     \n",
        "  smiles_array = smiles_to_array(smiles) \n",
        "\n",
        "  image = np.expand_dims(image, 0)\n",
        "  smiles_array = np.expand_dims(smiles_array, 0)       \n",
        "\n",
        "  value = best_global_model.predict((image, smiles_array)).argmax()   \n",
        "  predicted_test.append(value)   \n",
        "\n",
        "print(classification_report(test.classes.tolist(), predicted_test))    "
      ],
      "metadata": {
        "id": "ZXOwxj9sBL5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Kw9fcxklW2Y"
      },
      "outputs": [],
      "source": [
        "# references  \n",
        "# https://www.tensorflow.org/guide/data\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "# https://www.tensorflow.org/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}